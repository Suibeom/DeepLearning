# DeepLearning
I wrote these while working through Geoffry Hinton's deep learning Coursera course. At the time, there were no good deep learning frameworks for Julia and I wanted to learn the language, so I reimplemented the algorithms by hand (like you do, in a good course on algorithms..!)
I think they were written for Julia v0.4, and some aggressive refactoring has been done now that we're at v1.02 so some stuff probably needs to be tweaks to make them runnable on the latest stable Julia version.
Echo State Networks are antique technology now, but I thought they were fun. The idea was that in the early days when people were not having much success training RNNs, because they were too big and SGD optimizers were not very sophistocated, you would instead start with something that just fed forward internal states chaotically. So, the network would learn how to put things into the internal state, and how to read things out of it after it's fed forward, but it would NOT learn how to feed forward the internal states.

Same with Restricted Boltzmann Machines...
Factor RNN is no longer the widely used name for this network, but it's also not in fashion anymore. The idea was to cause an RNN to change the way it passed its hidden states forward depending on the current input. Most RNN architectures do NOT change the feedforward weights dynamically; the inputs interact with the hidden states additively. 
